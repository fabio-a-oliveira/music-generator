{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "missing-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from os import getcwd, listdir, mkdir\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "import mido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pregnant-denver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv(file):\n",
    "    dataset_parsed = tf.data.TextLineDataset(file).skip(1)\n",
    "    defaults = [0]*4\n",
    "    dataset_parsed = dataset_parsed.map(lambda line: tf.stack(tf.io.decode_csv(line, defaults)))\n",
    "    eof = tf.data.Dataset.from_tensor_slices([-1,-1,-1,-1]).batch(4)\n",
    "    dataset_parsed = dataset_parsed.concatenate(eof)\n",
    "    dataset_parsed = dataset_parsed.unbatch()\n",
    "    return dataset_parsed\n",
    "    \n",
    "def load_partition(partition='train'):\n",
    "    filename = listdir(join(getcwd(), 'bach-chorales', partition))\n",
    "    files = []\n",
    "    for file in filename:\n",
    "        files.append(join(getcwd(), 'bach-chorales', partition, file))\n",
    "    dataset = tf.data.Dataset.list_files(files)\n",
    "    dataset = dataset.flat_map(parse_csv) # flat_map ~ simple interleave\n",
    "    return dataset\n",
    "\n",
    "def one_hot_encoding(note):\n",
    "    vector = np.zeros((81-36+3))\n",
    "    if note == -1: # eof\n",
    "        vector[0] = 1\n",
    "    elif note == 0: # silence\n",
    "        vector[1] = 1\n",
    "    elif note >= 36 and note <= 81:\n",
    "        vector[note-36+2] = 1\n",
    "    else:\n",
    "        raise Exception('invalid note, should be either -1 (eof), 0 (silence), or [36,81]')\n",
    "    return vector\n",
    "\n",
    "def make_dataset(partition='train'):\n",
    "    dataset = list(load_partition(partition).as_numpy_iterator())\n",
    "    x = dataset[:-1]\n",
    "    y = dataset[1:]\n",
    "    x_encoded = [one_hot_encoding(note) for note in x]\n",
    "    y_encoded = [one_hot_encoding(note) for note in y]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_encoded,y_encoded))\n",
    "    return dataset\n",
    "\n",
    "def one_hot_decoding(vector):\n",
    "    if vector.flatten()[0] == 1: # eof\n",
    "        note = -1\n",
    "    elif vector.flatten()[1] == 1: # silence\n",
    "        note = 0\n",
    "    else:\n",
    "        note = vector.flatten().argmax() + 36 - 2\n",
    "    return note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "defensive-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = make_dataset('train'), make_dataset('valid'), make_dataset('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "adjacent-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2 .53 @test\n",
    "#model = keras.models.Sequential()\n",
    "#model.add(keras.layers.GRU(96, dropout = .1, recurrent_dropout = .1, input_shape = [None, 48], return_sequences = True))\n",
    "#model.add(keras.layers.GRU(96, dropout = .1, recurrent_dropout = .1, return_sequences = True))\n",
    "#model.add(keras.layers.GRU(96, dropout = .1, recurrent_dropout = .1, return_sequences = True))\n",
    "#model.add(keras.layers.Dense(48, kernel_regularizer = keras.regularizers.l2(l2=0)))\n",
    "#model.add(keras.layers.Dropout(.1))\n",
    "#model.add(keras.layers.Activation('softmax'))\n",
    "\n",
    "# model 3 - .34 @train, .53 @valid, .54 @test, without much tinkering\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.GRU(96, dropout = .1, recurrent_dropout = .1, input_shape = [None, 48], return_sequences = True))\n",
    "model.add(keras.layers.GRU(96, dropout = .1, recurrent_dropout = .1, return_sequences = True))\n",
    "model.add(keras.layers.GRU(96, dropout = .1, recurrent_dropout = .1, return_sequences = True))\n",
    "model.add(keras.layers.GRU(96, dropout = .1, recurrent_dropout = .1, return_sequences = True))\n",
    "model.add(keras.layers.Dense(48, kernel_regularizer = keras.regularizers.l2(l2=0)))\n",
    "model.add(keras.layers.Dropout(.1))\n",
    "model.add(keras.layers.Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "unexpected-burlington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "54/54 [==============================] - 194s 4s/step - loss: 0.6582 - val_loss: 0.5330\n",
      "Epoch 2/1000\n",
      "54/54 [==============================] - 194s 4s/step - loss: 0.6548 - val_loss: 0.5332\n",
      "Epoch 3/1000\n",
      "54/54 [==============================] - 190s 4s/step - loss: 0.6574 - val_loss: 0.5332\n",
      "Epoch 4/1000\n",
      "54/54 [==============================] - 190s 4s/step - loss: 0.6564 - val_loss: 0.5330\n",
      "Epoch 5/1000\n",
      "54/54 [==============================] - 195s 4s/step - loss: 0.6600 - val_loss: 0.5328\n",
      "Epoch 6/1000\n",
      "54/54 [==============================] - 191s 4s/step - loss: 0.6558 - val_loss: 0.5331\n",
      "Epoch 7/1000\n",
      "54/54 [==============================] - 189s 3s/step - loss: 0.6554 - val_loss: 0.5333\n",
      "Epoch 8/1000\n",
      "54/54 [==============================] - 190s 4s/step - loss: 0.6569 - val_loss: 0.5332\n",
      "Epoch 9/1000\n",
      "54/54 [==============================] - 185s 3s/step - loss: 0.6534 - val_loss: 0.5330\n",
      "Epoch 10/1000\n",
      "54/54 [==============================] - 183s 3s/step - loss: 0.6540 - val_loss: 0.5328\n",
      "Epoch 11/1000\n",
      "54/54 [==============================] - 188s 3s/step - loss: 0.6553 - val_loss: 0.5327\n",
      "Epoch 12/1000\n",
      "54/54 [==============================] - 187s 3s/step - loss: 0.6538 - val_loss: 0.5327\n",
      "Epoch 13/1000\n",
      "54/54 [==============================] - 185s 3s/step - loss: 0.6570 - val_loss: 0.5326\n",
      "Epoch 14/1000\n",
      "54/54 [==============================] - 188s 3s/step - loss: 0.6519 - val_loss: 0.5327\n",
      "Epoch 15/1000\n",
      "54/54 [==============================] - 185s 3s/step - loss: 0.6548 - val_loss: 0.5328\n",
      "Epoch 16/1000\n",
      "54/54 [==============================] - 181s 3s/step - loss: 0.6526 - val_loss: 0.5327\n",
      "Epoch 17/1000\n",
      "54/54 [==============================] - 187s 3s/step - loss: 0.6540 - val_loss: 0.5326\n",
      "Epoch 18/1000\n",
      "54/54 [==============================] - 188s 3s/step - loss: 0.6519 - val_loss: 0.5323\n",
      "Epoch 19/1000\n",
      "54/54 [==============================] - 183s 3s/step - loss: 0.6515 - val_loss: 0.5328\n",
      "Epoch 20/1000\n",
      "54/54 [==============================] - 188s 3s/step - loss: 0.6509 - val_loss: 0.5328\n",
      "Epoch 21/1000\n",
      "54/54 [==============================] - 190s 4s/step - loss: 0.6536 - val_loss: 0.5329\n",
      "Epoch 22/1000\n",
      "54/54 [==============================] - 185s 3s/step - loss: 0.6546 - val_loss: 0.5324\n",
      "Epoch 23/1000\n",
      "54/54 [==============================] - 185s 3s/step - loss: 0.6536 - val_loss: 0.5323\n",
      "Epoch 24/1000\n",
      "54/54 [==============================] - 188s 3s/step - loss: 0.6550 - val_loss: 0.5324\n",
      "Epoch 25/1000\n",
      "54/54 [==============================] - 184s 3s/step - loss: 0.6520 - val_loss: 0.5321\n",
      "Epoch 26/1000\n",
      "54/54 [==============================] - 185s 3s/step - loss: 0.6506 - val_loss: 0.5323\n",
      "Epoch 27/1000\n",
      "54/54 [==============================] - 190s 4s/step - loss: 0.6549 - val_loss: 0.5319\n",
      "Epoch 28/1000\n",
      "54/54 [==============================] - 187s 3s/step - loss: 0.6502 - val_loss: 0.5321\n",
      "Epoch 29/1000\n",
      "54/54 [==============================] - 184s 3s/step - loss: 0.6498 - val_loss: 0.5323\n",
      "Epoch 30/1000\n",
      "54/54 [==============================] - 187s 3s/step - loss: 0.6552 - val_loss: 0.5323\n",
      "Epoch 31/1000\n",
      "54/54 [==============================] - 187s 3s/step - loss: 0.6513 - val_loss: 0.5321\n",
      "Epoch 32/1000\n",
      "54/54 [==============================] - 181s 3s/step - loss: 0.6523 - val_loss: 0.5322\n",
      "Epoch 33/1000\n",
      "54/54 [==============================] - 188s 3s/step - loss: 0.6510 - val_loss: 0.5321\n",
      "Epoch 34/1000\n",
      "54/54 [==============================] - 187s 3s/step - loss: 0.6497 - val_loss: 0.5321\n",
      "Epoch 35/1000\n",
      "54/54 [==============================] - 186s 3s/step - loss: 0.6509 - val_loss: 0.5321\n",
      "Epoch 36/1000\n",
      "54/54 [==============================] - 186s 3s/step - loss: 0.6523 - val_loss: 0.5323\n",
      "Epoch 37/1000\n",
      "54/54 [==============================] - 187s 3s/step - loss: 0.6504 - val_loss: 0.5323\n",
      "Epoch 38/1000\n",
      "54/54 [==============================] - 184s 3s/step - loss: 0.6515 - val_loss: 0.5322\n",
      "Epoch 39/1000\n",
      "54/54 [==============================] - 184s 3s/step - loss: 0.6486 - val_loss: 0.5323\n",
      "Epoch 40/1000\n",
      "54/54 [==============================] - 189s 3s/step - loss: 0.6497 - val_loss: 0.5323\n",
      "Epoch 41/1000\n",
      "54/54 [==============================] - 184s 3s/step - loss: 0.6517 - val_loss: 0.5322\n",
      "Epoch 42/1000\n",
      "54/54 [==============================] - 185s 3s/step - loss: 0.6477 - val_loss: 0.5320\n",
      "Epoch 43/1000\n",
      "54/54 [==============================] - 190s 4s/step - loss: 0.6499 - val_loss: 0.5320\n",
      "Epoch 44/1000\n",
      "54/54 [==============================] - 186s 3s/step - loss: 0.6469 - val_loss: 0.5324\n",
      "Epoch 45/1000\n",
      "54/54 [==============================] - 185s 3s/step - loss: 0.6480 - val_loss: 0.5324\n",
      "Epoch 46/1000\n",
      "54/54 [==============================] - 188s 3s/step - loss: 0.6475 - val_loss: 0.5324\n",
      "Epoch 47/1000\n",
      "54/54 [==============================] - 186s 3s/step - loss: 0.6466 - val_loss: 0.5323\n",
      "Epoch 48/1000\n",
      "54/54 [==============================] - 183s 3s/step - loss: 0.6487 - val_loss: 0.5329\n",
      "Epoch 49/1000\n",
      "54/54 [==============================] - 187s 3s/step - loss: 0.6469 - val_loss: 0.5323\n",
      "Epoch 50/1000\n",
      "54/54 [==============================] - 187s 3s/step - loss: 0.6513 - val_loss: 0.5323\n",
      "Epoch 51/1000\n",
      "54/54 [==============================] - 181s 3s/step - loss: 0.6491 - val_loss: 0.5323\n",
      "Epoch 52/1000\n",
      "54/54 [==============================] - 185s 3s/step - loss: 0.6510 - val_loss: 0.5324\n",
      "Epoch 53/1000\n",
      "54/54 [==============================] - 188s 3s/step - loss: 0.6509 - val_loss: 0.5324\n",
      "Epoch 54/1000\n",
      "54/54 [==============================] - 184s 3s/step - loss: 0.6496 - val_loss: 0.5323\n",
      "Epoch 55/1000\n",
      "54/54 [==============================] - 185s 3s/step - loss: 0.6500 - val_loss: 0.5324\n",
      "Epoch 56/1000\n",
      "54/54 [==============================] - 188s 3s/step - loss: 0.6496 - val_loss: 0.5326\n",
      "Epoch 57/1000\n",
      "54/54 [==============================] - 183s 3s/step - loss: 0.6500 - val_loss: 0.5326\n",
      "Epoch 58/1000\n",
      "54/54 [==============================] - 184s 3s/step - loss: 0.6485 - val_loss: 0.5324\n",
      "Epoch 59/1000\n",
      "54/54 [==============================] - 190s 4s/step - loss: 0.6506 - val_loss: 0.5319\n",
      "Epoch 60/1000\n",
      "54/54 [==============================] - 186s 3s/step - loss: 0.6508 - val_loss: 0.5321\n",
      "Epoch 61/1000\n",
      "54/54 [==============================] - 184s 3s/step - loss: 0.6466 - val_loss: 0.5323\n",
      "Epoch 62/1000\n",
      "54/54 [==============================] - 188s 3s/step - loss: 0.6449 - val_loss: 0.5322\n",
      "Epoch 63/1000\n",
      "54/54 [==============================] - 189s 3s/step - loss: 0.6461 - val_loss: 0.5322\n",
      "Epoch 64/1000\n",
      "54/54 [==============================] - 185s 3s/step - loss: 0.6490 - val_loss: 0.5322\n",
      "Epoch 65/1000\n",
      "54/54 [==============================] - 188s 3s/step - loss: 0.6458 - val_loss: 0.5324\n",
      "Epoch 66/1000\n",
      "54/54 [==============================] - 187s 3s/step - loss: 0.6467 - val_loss: 0.5325\n",
      "Epoch 67/1000\n",
      "54/54 [==============================] - 184s 3s/step - loss: 0.6489 - val_loss: 0.5326\n",
      "Epoch 68/1000\n",
      "54/54 [==============================] - 187s 3s/step - loss: 0.6475 - val_loss: 0.5329\n",
      "Epoch 69/1000\n",
      "54/54 [==============================] - 187s 3s/step - loss: 0.6472 - val_loss: 0.5326\n",
      "Epoch 70/1000\n",
      "54/54 [==============================] - 183s 3s/step - loss: 0.6465 - val_loss: 0.5327\n",
      "Epoch 71/1000\n",
      "54/54 [==============================] - 184s 3s/step - loss: 0.6468 - val_loss: 0.5325\n",
      "Epoch 72/1000\n",
      "54/54 [==============================] - 188s 3s/step - loss: 0.6485 - val_loss: 0.5324\n",
      "Epoch 73/1000\n",
      "54/54 [==============================] - 185s 3s/step - loss: 0.6475 - val_loss: 0.5325\n",
      "Epoch 74/1000\n",
      "54/54 [==============================] - 185s 3s/step - loss: 0.6488 - val_loss: 0.5319\n",
      "Epoch 75/1000\n",
      "54/54 [==============================] - 189s 3s/step - loss: 0.6451 - val_loss: 0.5321\n",
      "Epoch 76/1000\n",
      "54/54 [==============================] - 185s 3s/step - loss: 0.6434 - val_loss: 0.5325\n",
      "Epoch 77/1000\n",
      "54/54 [==============================] - ETA: 0s - loss: 0.6476Restoring model weights from the end of the best epoch.\n",
      "54/54 [==============================] - 186s 3s/step - loss: 0.6476 - val_loss: 0.5323\n",
      "Epoch 00077: early stopping\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001 # 0.01 -> 0.001 -> 0.0001\n",
    "sequence_length = 256 # 256\n",
    "batch_size = 16 # 16 -> 16 -> 64\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss = 'categorical_crossentropy')\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=50, restore_best_weights=True, verbose=True)\n",
    "\n",
    "# PROBABLY SHOULDN'T BATCH THE VALIDATION SET! longer sequences tend to provide better prediction...\n",
    "\n",
    "history = model.fit(train.batch(sequence_length).batch(batch_size, drop_remainder=True).cache(),\n",
    "                    epochs = 1000,\n",
    "                    validation_data = valid.batch(sequence_length).batch(batch_size, drop_remainder=True).cache(),\n",
    "                    callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "capital-township",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 31s 570ms/step - loss: 0.3473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3473449647426605"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(train.batch(sequence_length).batch(batch_size, drop_remainder=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "informative-bible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 10s 544ms/step - loss: 0.5319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5318822860717773"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid.batch(sequence_length).batch(batch_size, drop_remainder=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "union-speed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 10s 536ms/step - loss: 0.5420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5420477390289307"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test.batch(sequence_length).batch(batch_size, drop_remainder=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "manual-excess",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1310669db20>]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "functioning-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# music generation\n",
    "\n",
    "Length = 800 # number of new notes on all voices, actual piece length is Length/4\n",
    "Temperature = 1.0 # not really temperature (which should be applied before softmax), but works similarly (smaller values are more adventurous, larger values are more conservative)\n",
    "\n",
    "song = []\n",
    "song_encoded = one_hot_encoding(-1).reshape((1,1,48)).repeat(16, axis=1)\n",
    "song_encoded.shape\n",
    "\n",
    "new_note = -1\n",
    "new_note_encoding = one_hot_encoding(new_note).reshape((1,1,48))\n",
    "\n",
    "# music generation loop\n",
    "\n",
    "for _ in range(Length):\n",
    "    p = model.predict(song_encoded)[:,-1,:].flatten() # the model is reading the entire chorale up to this point, which makes it very slow for long sequences. This should be improved, providing a limited number os samples for each new note or making the network remember the last state.\n",
    "    p = p**Temperature / (p**Temperature).sum()\n",
    "    \n",
    "    out = np.random.choice(48, p=p)\n",
    "    new_note_encoding = np.zeros((1,1,48))\n",
    "    new_note_encoding[0,0,out] = 1\n",
    "    new_note = one_hot_decoding(new_note_encoding)\n",
    "\n",
    "    song.append(new_note)\n",
    "    extended_song = np.zeros((song_encoded.shape[0], song_encoded.shape[1]+1, song_encoded.shape[2]))\n",
    "    extended_song[:,:-1,:] = song_encoded\n",
    "    extended_song[:,-1,:] = new_note_encoding\n",
    "    song_encoded = extended_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "satellite-freight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[65, 60, 57, 53],\n",
       "        [65, 60, 57, 53],\n",
       "        [67, 62, 58, 53],\n",
       "        [67, 62, 58, 53]],\n",
       "\n",
       "       [[69, 63, 60, 53],\n",
       "        [69, 63, 60, 53],\n",
       "        [69, 62, 60, 53],\n",
       "        [69, 62, 60, 53]],\n",
       "\n",
       "       [[70, 62, 58, 55],\n",
       "        [70, 62, 58, 55],\n",
       "        [70, 62, 57, 55],\n",
       "        [70, 62, 57, 55]],\n",
       "\n",
       "       [[72, 60, 55, 51],\n",
       "        [72, 60, 55, 51],\n",
       "        [72, 70, 55, 51],\n",
       "        [72, 69, 55, 51]],\n",
       "\n",
       "       [[72, 69, 65, 50],\n",
       "        [72, 69, 65, 50],\n",
       "        [72, 69, 65, 50],\n",
       "        [72, 69, 65, 50]],\n",
       "\n",
       "       [[72, 67, 63, 48],\n",
       "        [72, 67, 63, 48],\n",
       "        [72, 67, 63, 48],\n",
       "        [72, 67, 63, 48]],\n",
       "\n",
       "       [[74, 65, 62, 46],\n",
       "        [74, 65, 62, 46],\n",
       "        [74, 65, 62, 46],\n",
       "        [74, 65, 62, 46]],\n",
       "\n",
       "       [[75, 67, 58, 51],\n",
       "        [75, 67, 58, 51],\n",
       "        [75, 67, 58, 51],\n",
       "        [75, 67, 58, 51]],\n",
       "\n",
       "       [[72, 65, 57, 53],\n",
       "        [72, 65, 57, 53],\n",
       "        [72, 65, 57, 53],\n",
       "        [72, 65, 57, 53]],\n",
       "\n",
       "       [[74, 65, 58, 46],\n",
       "        [74, 65, 58, 46],\n",
       "        [74, 65, 58, 46],\n",
       "        [74, 65, 58, 46]],\n",
       "\n",
       "       [[72, 65, 57, 53],\n",
       "        [72, 65, 57, 53],\n",
       "        [72, 65, 57, 53],\n",
       "        [72, 65, 57, 53]],\n",
       "\n",
       "       [[74, 65, 58, 58],\n",
       "        [74, 65, 58, 58],\n",
       "        [74, 65, 58, 58],\n",
       "        [74, 65, 58, 58]],\n",
       "\n",
       "       [[74, 66, 58, 58],\n",
       "        [74, 66, 58, 58],\n",
       "        [74, 63, 58, 58],\n",
       "        [74, 63, 58, 58]],\n",
       "\n",
       "       [[70, 63, 60, 55],\n",
       "        [70, 63, 60, 55],\n",
       "        [70, 63, 60, 55],\n",
       "        [70, 63, 60, 55]],\n",
       "\n",
       "       [[74, 65, 58, 53],\n",
       "        [74, 65, 58, 53],\n",
       "        [74, 65, 58, 53],\n",
       "        [74, 65, 58, 53]],\n",
       "\n",
       "       [[74, 65, 58, 53],\n",
       "        [74, 65, 58, 53],\n",
       "        [74, 65, 58, 51],\n",
       "        [74, 65, 58, 51]],\n",
       "\n",
       "       [[72, 65, 57, 53],\n",
       "        [72, 65, 57, 53],\n",
       "        [72, 65, 57, 53],\n",
       "        [72, 65, 57, 53]],\n",
       "\n",
       "       [[70, 65, 62, 46],\n",
       "        [70, 65, 62, 46],\n",
       "        [70, 65, 62, 46],\n",
       "        [70, 65, 62, 46]],\n",
       "\n",
       "       [[70, 65, 62, 46],\n",
       "        [70, 65, 62, 46],\n",
       "        [70, 65, 62, 46],\n",
       "        [70, 65, 62, 46]],\n",
       "\n",
       "       [[72, 65, 60, 45],\n",
       "        [72, 65, 60, 45],\n",
       "        [72, 65, 60, 45],\n",
       "        [72, 65, 60, 45]],\n",
       "\n",
       "       [[70, 65, 62, 46],\n",
       "        [70, 65, 62, 46],\n",
       "        [70, 65, 62, 46],\n",
       "        [70, 65, 62, 46]],\n",
       "\n",
       "       [[72, 65, 57, 53],\n",
       "        [72, 65, 57, 53],\n",
       "        [72, 65, 57, 51],\n",
       "        [72, 65, 57, 51]],\n",
       "\n",
       "       [[70, 65, 58, 50],\n",
       "        [70, 65, 58, 50],\n",
       "        [70, 65, 58, 50],\n",
       "        [70, 65, 58, 50]],\n",
       "\n",
       "       [[72, 67, 63, 48],\n",
       "        [72, 67, 63, 48],\n",
       "        [72, 67, 63, 48],\n",
       "        [72, 67, 63, 48]],\n",
       "\n",
       "       [[72, 69, 63, 48],\n",
       "        [72, 69, 63, 48],\n",
       "        [72, 69, 63, 48],\n",
       "        [72, 69, 63, 48]],\n",
       "\n",
       "       [[70, 67, 62, 55],\n",
       "        [70, 67, 62, 55],\n",
       "        [70, 67, 62, 55],\n",
       "        [70, 67, 62, 55]],\n",
       "\n",
       "       [[70, 67, 62, 55],\n",
       "        [70, 67, 62, 55],\n",
       "        [70, 67, 62, 55],\n",
       "        [70, 67, 62, 55]],\n",
       "\n",
       "       [[72, 67, 63, 48],\n",
       "        [72, 67, 63, 48],\n",
       "        [72, 67, 63, 48],\n",
       "        [72, 67, 63, 48]],\n",
       "\n",
       "       [[74, 67, 58, 50],\n",
       "        [74, 67, 58, 50],\n",
       "        [74, 67, 58, 50],\n",
       "        [74, 67, 58, 50]],\n",
       "\n",
       "       [[75, 67, 58, 51],\n",
       "        [75, 67, 58, 51],\n",
       "        [75, 67, 58, 51],\n",
       "        [75, 67, 58, 51]],\n",
       "\n",
       "       [[75, 67, 58, 51],\n",
       "        [75, 67, 58, 51],\n",
       "        [75, 67, 58, 51],\n",
       "        [75, 67, 58, 51]],\n",
       "\n",
       "       [[75, 65, 60, 57],\n",
       "        [75, 65, 60, 57],\n",
       "        [74, 65, 58, 58],\n",
       "        [74, 65, 58, 58]],\n",
       "\n",
       "       [[72, 72, 58, 51],\n",
       "        [72, 70, 58, 51],\n",
       "        [72, 70, 65, 51],\n",
       "        [72, 70, 65, 51]],\n",
       "\n",
       "       [[70, 70, 65, 50],\n",
       "        [70, 70, 65, 50],\n",
       "        [70, 70, 65, 50],\n",
       "        [70, 70, 65, 50]],\n",
       "\n",
       "       [[75, 70, 67, 48],\n",
       "        [75, 70, 67, 48],\n",
       "        [75, 70, 67, 48],\n",
       "        [75, 70, 67, 48]],\n",
       "\n",
       "       [[75, 69, 65, 53],\n",
       "        [75, 69, 65, 53],\n",
       "        [74, 70, 65, 58],\n",
       "        [74, 70, 65, 58]],\n",
       "\n",
       "       [[74, 70, 65, 58],\n",
       "        [74, 70, 65, 58],\n",
       "        [74, 70, 65, 58],\n",
       "        [74, 70, 65, 58]],\n",
       "\n",
       "       [[74, 65, 65, 58],\n",
       "        [74, 65, 65, 58],\n",
       "        [74, 67, 63, 58],\n",
       "        [74, 67, 63, 58]],\n",
       "\n",
       "       [[72, 63, 58, 56],\n",
       "        [72, 63, 58, 56],\n",
       "        [72, 62, 60, 56],\n",
       "        [72, 62, 60, 56]],\n",
       "\n",
       "       [[72, 65, 56, 53],\n",
       "        [72, 65, 56, 53],\n",
       "        [72, 63, 56, 53],\n",
       "        [72, 63, 56, 53]],\n",
       "\n",
       "       [[70, 62, 58, 58],\n",
       "        [70, 62, 58, 58],\n",
       "        [70, 62, 58, 58],\n",
       "        [70, 62, 58, 58]],\n",
       "\n",
       "       [[72, 67, 58, 51],\n",
       "        [72, 67, 58, 51],\n",
       "        [72, 67, 58, 51],\n",
       "        [72, 67, 58, 51]],\n",
       "\n",
       "       [[72, 63, 57, 53],\n",
       "        [72, 63, 57, 53],\n",
       "        [72, 62, 57, 53],\n",
       "        [72, 62, 57, 53]],\n",
       "\n",
       "       [[74, 65, 58, 46],\n",
       "        [74, 65, 58, 46],\n",
       "        [74, 65, 58, 46],\n",
       "        [74, 65, 58, 46]],\n",
       "\n",
       "       [[74, 65, 58, 46],\n",
       "        [74, 65, 58, 46],\n",
       "        [74, 65, 58, 46],\n",
       "        [74, 65, 58, 46]],\n",
       "\n",
       "       [[72, 65, 57, 48],\n",
       "        [72, 65, 57, 48],\n",
       "        [72, 65, 57, 53],\n",
       "        [72, 65, 57, 53]],\n",
       "\n",
       "       [[70, 65, 62, 46],\n",
       "        [70, 65, 62, 46],\n",
       "        [70, 65, 62, 46],\n",
       "        [70, 65, 62, 46]],\n",
       "\n",
       "       [[68, 65, 60, 46],\n",
       "        [68, 65, 60, 46],\n",
       "        [68, 65, 58, 48],\n",
       "        [68, 65, 58, 48]],\n",
       "\n",
       "       [[67, 63, 58, 46],\n",
       "        [67, 63, 58, 46],\n",
       "        [67, 63, 58, 46],\n",
       "        [67, 63, 58, 46]],\n",
       "\n",
       "       [[69, 65, 58, 44],\n",
       "        [69, 65, 58, 44],\n",
       "        [70, 65, 58, 46],\n",
       "        [70, 65, 58, 46]]], dtype=int64)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(song).reshape((-1,4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "north-tsunami",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note1</th>\n",
       "      <th>note2</th>\n",
       "      <th>note3</th>\n",
       "      <th>note4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>60</td>\n",
       "      <td>57</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65</td>\n",
       "      <td>60</td>\n",
       "      <td>57</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>62</td>\n",
       "      <td>58</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67</td>\n",
       "      <td>62</td>\n",
       "      <td>58</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69</td>\n",
       "      <td>63</td>\n",
       "      <td>60</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>67</td>\n",
       "      <td>63</td>\n",
       "      <td>58</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>67</td>\n",
       "      <td>63</td>\n",
       "      <td>58</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>69</td>\n",
       "      <td>65</td>\n",
       "      <td>58</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>69</td>\n",
       "      <td>65</td>\n",
       "      <td>58</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>70</td>\n",
       "      <td>65</td>\n",
       "      <td>58</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     note1  note2  note3  note4\n",
       "0       65     60     57     53\n",
       "1       65     60     57     53\n",
       "2       67     62     58     53\n",
       "3       67     62     58     53\n",
       "4       69     63     60     53\n",
       "..     ...    ...    ...    ...\n",
       "194     67     63     58     46\n",
       "195     67     63     58     46\n",
       "196     69     65     58     44\n",
       "197     69     65     58     44\n",
       "198     70     65     58     46\n",
       "\n",
       "[199 rows x 4 columns]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clip entries before beginning\n",
    "\n",
    "first_note_position = 0\n",
    "for position, note in enumerate(song):\n",
    "    if note != -1:\n",
    "        first_note_position = position\n",
    "        break\n",
    "\n",
    "new_chorale = np.array(song[first_note_position : -4 + (first_note_position % 4)]).reshape(-1,4)\n",
    "        \n",
    "# clip entries after eof\n",
    "\n",
    "first_eof = (new_chorale.min(axis = 1) == -1).astype('int').argmax()\n",
    "if first_eof > 0:\n",
    "    new_chorale = new_chorale[:first_eof, :]\n",
    "        \n",
    "# convert finished product to data frame\n",
    "\n",
    "new_chorale = pd.DataFrame(new_chorale, columns=['note1','note2','note3','note4'])\n",
    "new_chorale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "iraqi-aircraft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# music generation with legato (attack only when new note is different from current one)\n",
    "\n",
    "new_chorale_midi = mido.MidiFile(type=1)\n",
    "\n",
    "time_unit = 360\n",
    "instrument = 52 # choir aahs\n",
    "volume = [50, 50, 50, 50] # volume for each channel\n",
    "\n",
    "for channel in range(4):\n",
    "    track = mido.MidiTrack()\n",
    "    track.append(mido.Message('program_change', channel = channel, program = instrument, time = 0))\n",
    "    previous_note = 0\n",
    "    steps = 1\n",
    "    \n",
    "    for pos, note in enumerate(new_chorale.iloc[:, channel]):\n",
    "        if note == previous_note:\n",
    "            steps += 1\n",
    "        if note != previous_note:\n",
    "            if pos != 0:\n",
    "                track.append(mido.Message('note_off', channel=channel, note=previous_note, time = steps * time_unit))\n",
    "            track.append(mido.Message('note_on', channel=channel, note=note, velocity=volume[channel], time=0))\n",
    "            previous_note = note\n",
    "            steps = 1\n",
    "            \n",
    "    new_chorale_midi.tracks.append(track)\n",
    "\n",
    "new_chorale_midi.save('new_chorale.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-password",
   "metadata": {},
   "source": [
    "### Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1 - 1-layer SimpleRNN or GRU\n",
    "#model = keras.models.Sequential()\n",
    "#model.add(keras.layers.SimpleRNN(48, input_shape=[None,48], return_sequences=True, activation='softmax'))\n",
    "#model.add(keras.layers.GRU(48, input_shape=[None,48], return_sequences=True, activation='softmax'))\n",
    "\n",
    "# model 3 - with embedding, without encoding (will not work because of how the dataset was set up)\n",
    "#model = keras.models.Sequential()\n",
    "#model.add(keras.layers.Embedding(48,24)) # embedding requires integers, not one-hot\n",
    "#model.add(keras.layers.GRU(48, return_sequences = True))\n",
    "#model.add(keras.layers.GRU(48, return_sequences = True))\n",
    "#model.add(keras.layers.GRU(48, return_sequences = True, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-collins",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "refined-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# music generation with 1 attack per chord (not very cool...)\n",
    "\n",
    "#new_chorale_midi = mido.MidiFile()\n",
    "#new_chorale_track = mido.MidiTrack()\n",
    "#\n",
    "#time_unit = 120\n",
    "#instrument = 52 # choir aahs\n",
    "#\n",
    "## set the instrument\n",
    "#for channel in range(4):\n",
    "#    new_chorale_track.append(mido.Message('program_change', channel=channel, program=instrument, time=0))\n",
    "#    \n",
    "#for line in range(new_chorale.shape[0]):\n",
    "#    notes = new_chorale.loc[line]\n",
    "#    for channel, note in enumerate(notes):\n",
    "#        new_chorale_track.append(mido.Message('note_on', note=note, velocity=100, channel=channel, time=0))\n",
    "#        previous_note = note\n",
    "#    previous_notes = notes\n",
    "#    for channel, note in enumerate(notes):\n",
    "#        new_chorale_track.append(mido.Message('note_off', note=previous_notes[channel], channel=channel, time=time_unit))\n",
    "#        #new_chorale_track.append(mido.Message('note_on', note=note, velocity=0, channel=channel, time=time_unit))\n",
    "#\n",
    "#new_chorale_midi.tracks.append(new_chorale_track)\n",
    "#\n",
    "#new_chorale_midi.save('new_chorale.mid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
