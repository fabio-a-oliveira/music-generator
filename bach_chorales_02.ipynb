{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "consolidated-message",
   "metadata": {},
   "source": [
    "## Creating original computer-generated music inspired by Bach's chorales\n",
    "\n",
    "In this notebook, we create a model that will be able to generate original music after being trained on a large corpus comprised of J.S. Bach's chorale pieces.   \n",
    "\n",
    "Some high-level features of this work: \n",
    "* This project is inspired by an exercise proposed in chapter 15 of the _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_ book. You can check out the author's implementation in [this notebook](https://github.com/ageron/handson-ml2/blob/master/15_processing_sequences_using_rnns_and_cnns.ipynb)      \n",
    "* The model is based on an architecture consisting of stacked Gated Recurrent Units followed by a dense layer, regularized with droupout, and stabilized with layer normalization,          \n",
    "* Training is performed in a many-to-many scheme: at each step, the model tries to predict the next note (as opposed to a many-to-one scheme, where the model would be exposed to a sequence of notes and try to predict the last one)   \n",
    "* The dataset is available at [this link](https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/jsb_chorales/jsb_chorales.tgz) and contains _.csv_ files for each of the pieces    \n",
    "* The dataset is split into train (229 pieces), validation (76 pieces) and test (77 pieces) partitions (yes, Bach was a tremendous workaholic!)   \n",
    "* Each sample in the _.csv_ files contains 4 integer values ranging from 36 to 81 corresponding to 4 notes (one for each of the four voices) that are sung simultaneously, with an uniform note duration of half a beat      \n",
    "* In order to train the model, each note is converted to a one-hot encoding to represent each of the possible notes, plus 0 for silence and -1 to indicate the end of a piece\n",
    "* The dataset is augmented so that, before each training epoch, each of the pieces is randomly transposed by a number of steps in the range of -5 to 6 - this ensures that the model is equally exposed to musical patterns in all different keys   \n",
    "* After training, the model is used to generate new music one note at a time by choosing each note randomly according to the probabilities generated by the softmax layer of the neural network      \n",
    "* The original music is saved to midi files using the _mido_ package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-speech",
   "metadata": {},
   "source": [
    "### Importing packages\n",
    "\n",
    "Besides the usual suspects, we also import the _mido_ package for creating and saving the _.mid_ files, as well as some functions from _os_ and _os.path_ to manipulate files and directories.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adolescent-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "from os import getcwd, listdir, mkdir\n",
    "from os.path import join\n",
    "import mido\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-remainder",
   "metadata": {},
   "source": [
    "### Dataset preparation   \n",
    "\n",
    "The dataset is downloaded from [this link](https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/jsb_chorales/jsb_chorales.tgz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "compliant-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### download/unzip dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-bunny",
   "metadata": {},
   "source": [
    "Some helper functions are required to encode the music using one-hot encoding and decode the results from the softmax layer. Another helper function is defined to transpose an entire piece by a selected number of half steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dedicated-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 81+6 and 36-5 due to transposing randomly between -5 half-steps and +6 half-steps\n",
    "\n",
    "def one_hot_encoding(note):\n",
    "    vector = np.zeros(((81+6)-(36-5)+3))\n",
    "    if note == -1: # eof\n",
    "        vector[0] = 1\n",
    "    elif note == 0: # silence\n",
    "        vector[1] = 1\n",
    "    elif note >= (36-5) and note <= (81+6):\n",
    "        vector[note-(36-5)+2] = 1\n",
    "    else:\n",
    "        raise Exception('invalid note, should be either -1 (eof), 0 (silence), or [36-5,81+6]; received {}'.format(note))\n",
    "    return vector\n",
    "\n",
    "def one_hot_decoding(vector):\n",
    "    if vector.flatten()[0] == 1: # eof\n",
    "        note = -1\n",
    "    elif vector.flatten()[1] == 1: # silence\n",
    "        note = 0\n",
    "    else:\n",
    "        note = vector.flatten().argmax() + (36-5) - 2\n",
    "    return note\n",
    "\n",
    "def transpose_note(note, interval):\n",
    "    if note >= (36-5) and note <= (81+6):\n",
    "        transposed = note + interval\n",
    "    else:\n",
    "        transposed = note\n",
    "    return transposed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-functionality",
   "metadata": {},
   "source": [
    "Next, some functions are defined to systematically prepared the train, valid and test datasets using tensorflow's data API.   \n",
    "\n",
    "The make_dataset() function calls load_partition() to create a dataset consisting of a list of the files in each partition. This function then zips together each file and a randomly selected number of steps by which the piece will be transposed.   \n",
    "\n",
    "Since the number of steps is randomly generated via a generator function, this quantity will be different for each piece and will be resampled at each epoch. This ensures that the model will be trained with each of the pieces transposed to several different keys, ensuring variability and improving generalization.   \n",
    "\n",
    "After that, the parse_csv() function is called on each of the dataset entries (at this point, a file and a number of steps to transpose it by) via the flat_map() method. The parse_csv() function reads the _.csv_ file associated with one piece, decodes it, transposes all the notes and adds four end-of-file identifiers (-1).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "identical-coupon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_random_transpose():\n",
    "    yield tf.random.Generator.from_non_deterministic_state().uniform(shape=[], minval=-5, maxval=6+1, dtype=tf.dtypes.int32).numpy()    \n",
    "\n",
    "def parse_csv(file, interval): # added interval argument\n",
    "    dataset_parsed = tf.data.TextLineDataset(file).skip(1)\n",
    "    defaults = [0]*4\n",
    "    dataset_parsed = dataset_parsed.map(lambda line: tf.stack(tf.io.decode_csv(line, defaults)))\n",
    "    \n",
    "    dataset_parsed = dataset_parsed.unbatch()    \n",
    "    dataset_interval = tf.data.Dataset.from_tensor_slices([interval]).repeat()\n",
    "    dataset_parsed = tf.data.Dataset.zip((dataset_parsed, dataset_interval))\n",
    "    dataset_transposed = dataset_parsed.map(transpose_note)\n",
    "    \n",
    "    eof = tf.data.Dataset.from_tensor_slices([-1,-1,-1,-1])\n",
    "    dataset_transposed = dataset_transposed.concatenate(eof)\n",
    "    \n",
    "    return dataset_transposed\n",
    "\n",
    "def load_partition(partition='train'):\n",
    "    filename = listdir(join(getcwd(), 'bach-chorales', partition))\n",
    "    files = []\n",
    "    for file in filename:\n",
    "        files.append(join(getcwd(), 'bach-chorales', partition, file))\n",
    "    dataset = tf.data.Dataset.list_files(files)\n",
    "    \n",
    "    dataset_random_transpose = tf.data.Dataset.from_generator(generator_random_transpose, output_types=tf.dtypes.int32)\n",
    "    num_pieces = dataset.cardinality().numpy()\n",
    "    dataset = tf.data.Dataset.zip((dataset, dataset_random_transpose.repeat(num_pieces)))\n",
    "    \n",
    "    dataset = dataset.flat_map(parse_csv) # flat_map ~ simple interleave\n",
    "    return dataset\n",
    "\n",
    "def make_dataset(partition='train'):\n",
    "    dataset = list(load_partition(partition).as_numpy_iterator())\n",
    "    x = dataset[:-1]\n",
    "    y = dataset[1:]\n",
    "    x_encoded = [one_hot_encoding(note) for note in x]\n",
    "    y_encoded = [one_hot_encoding(note) for note in y]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_encoded, y_encoded))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-testimony",
   "metadata": {},
   "source": [
    "The make_dataset() function is called to prepare the train, valid and test sets. At this point, each dataset is composed of a sequence of single notes represented via a one-hot encoding. Splitting each into a number of different sequences and then into batches is done in the call to the model.fit() method, so that we can experiment with the sequence length and number samples per batch.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sought-worry",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = make_dataset('train'), make_dataset('valid'), make_dataset('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-belief",
   "metadata": {},
   "source": [
    "### Defining and training the model\n",
    "\n",
    "The model is composed of a 3 stacked GRU units followed by a dense output layer with softmax activation. The inputs and outpus are of dimension 59 (corresponding to 57 possible notes plus the silence and end-of-file indications). The GRUs are twice as wide as the input and output dimension.\n",
    "\n",
    "Because the model operates in a many-to-many scheme, every GRU layer returns its full sequence, which is used by the next layer.\n",
    "\n",
    "The model benefits significantly from regularization, both in the form of dropout (present in both the memory and input channels of each GRU and before the dense layer) and L2 regularization.\n",
    "\n",
    "Since this model is reasonably deep and deals with very long sequences, layer normalization is added to provide more stability during training.\n",
    "\n",
    "We use keras' Sequential API to specify the model (as the model is linear, without any skip connections)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "norwegian-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2 -\n",
    "#.47->.34 @train/ .65->.59 @valid/ .67->.62 @test\n",
    "# with lr scheduling: .30 @train / .60 @valid / .65 @test\n",
    "# with dropout .2, .2, .2: .38 @train / .58 @valid / .58 @test\n",
    "# with dropout .3, .3, .2:\n",
    "\n",
    "input_dim = 46 + 2 + 11 # 46 notes + 2 controls (silence & eof) + 11 transposing half-steps\n",
    "\n",
    "rec_dropout, gru_dropout, dense_dropout, l2_penalty = .3, .3, .2, 0\n",
    "\n",
    "model = keras.models.Sequential(name='model')\n",
    "model.add(keras.layers.GRU(2 * input_dim, dropout = gru_dropout, recurrent_dropout = rec_dropout, \n",
    "                           input_shape = [None, input_dim], return_sequences = True, name='GRU1'))\n",
    "model.add(keras.layers.GRU(2 * input_dim, dropout = gru_dropout, recurrent_dropout = rec_dropout, \n",
    "                           return_sequences = True, name='GRU2'))\n",
    "model.add(keras.layers.GRU(2 * input_dim, dropout = gru_dropout, recurrent_dropout = rec_dropout, \n",
    "                           return_sequences = True, name='GRU3'))\n",
    "model.add(keras.layers.Dropout(dense_dropout, name='Dropout1'))\n",
    "model.add(keras.layers.Dense(input_dim, activation='softmax', \n",
    "                             kernel_regularizer = keras.regularizers.l2(l2 = l2_penalty), name='Dense1'))\n",
    "\n",
    "\n",
    "# plan for model 2\n",
    "# dropout before dense layer (duh) - .47->.34 @train/ .65->.59 @valid/ .67->.62 @test\n",
    "# 4 bars of -1 - .48->.29 @train/ .65->.59 @valid/ .66->.59 @test / too many -1, songs too short, not very precise in outputing exactly 16\n",
    "# remove dropout from output layer: .50->.?? @train/ .66->.?? @valid/ .69->.?? @test\n",
    "# learning rate decay - did not improve results, but was able to approximate the results of 2 separate loops\n",
    "# shuffle after 1st batch (shuffle the sequences so that GD does not receive biased sequences) - only if not using stateful RNN\n",
    "# increase dropout on recurrent layers\n",
    "# layer normalization on GRUs\n",
    "# batch normalization on dense\n",
    "# regularization parameters tuning\n",
    "# change optimizers - decrease inertia\n",
    "# stateful RNN \n",
    "    #(to learn longer paterns, maybe predict next entry without going through entire sequence) - \n",
    "    # not so simple, batching is complicated\n",
    "    # create a stateful NN and copy the weights from the trained model, use it to generate new music faster\n",
    "    # with a stateful RNN, try to implement MC dropout\n",
    "# gradient clipping to avoid large peaks in loss functions - monitor gradients to see what is happening in those peaks\n",
    "# consider adding dense layers (too few parameters in comparison to GRUs)\n",
    "\n",
    "\n",
    "#: .??->.?? @train/ .??->.?? @valid/ .??->.?? @test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-practice",
   "metadata": {},
   "source": [
    "The model is trained with ADAM optimizer, learning rate scheduler function using power scheduling and reduce on plateau callback     \n",
    "\n",
    "Power scheduling reduces to 1/2 after 1 cycle, then 1/3 after 2 cycles, then 1/4 after 3 cycles etc. Stronger reduction in the beginning is adequate, so the training can begin with a high learning rate that ensures that the easier patterns (such as the bias due to imbalanced classes) are recognized quicker. The reduce on plateau is more active mid-training, reducing by a fixed ratio every time there is no improvement to the validation loss for N consecutive epochs.\n",
    "\n",
    "Tensorboard callback to save performance at the end of each epoch    \n",
    "\n",
    "Early stopping and restore_best_weights to ensure that the final model contains the weights that minimized the loss function on the validation set    \n",
    "\n",
    "The appropriate loss is categorical cross entropy - related to the model's predicted probability of the selected note. Accuracy is also measured.\n",
    "\n",
    "The model contains around 240,000 trainable parameters.\n",
    "\n",
    "Both the train and valid sets are batched in the call to the fit() method    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "noticed-communication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "GRU1 (GRU)                   (None, None, 118)         63366     \n",
      "_________________________________________________________________\n",
      "GRU2 (GRU)                   (None, None, 118)         84252     \n",
      "_________________________________________________________________\n",
      "GRU3 (GRU)                   (None, None, 118)         84252     \n",
      "_________________________________________________________________\n",
      "Dropout1 (Dropout)           (None, None, 118)         0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, None, 59)          7021      \n",
      "=================================================================\n",
      "Total params: 238,891\n",
      "Trainable params: 238,891\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# First learning loop\n",
    "\n",
    "sequence_length = 256\n",
    "batch_size = 16\n",
    "\n",
    "# setup learning rate scheduler\n",
    "\n",
    "lr_0 = 0.01\n",
    "lr_decay_rate = 1\n",
    "lr_decay_step = 40 # 1 update per epoch\n",
    "\n",
    "def lr_scheduler_fn(epoch, lr):\n",
    "    #new_lr = lr_0 / (1 + lr_decay_rate*epoch/lr_decay_step) # not cool, depends on lr_0 instead of current lr, does not play well with reduce on plateau callback\n",
    "    new_lr = lr * (1 + lr_decay_rate*epoch/lr_decay_step) / (1 + lr_decay_rate*(epoch+1)/lr_decay_step) # current lr time ratio between current and previous lr\n",
    "    print('Epoch {} / current learning rate: {} / new learning rate: {}'.format(epoch,np.round(lr,6),np.round(new_lr,6)))\n",
    "    return new_lr\n",
    "\n",
    "lr_scheduler_callback = keras.callbacks.LearningRateScheduler(lr_scheduler_fn)\n",
    "\n",
    "# setup for early stopping and tensorboard callbacks\n",
    "\n",
    "if 'tb_logs' not in listdir(join(getcwd(), 'bach-chorales')):\n",
    "    mkdir(join(getcwd(), 'bach-chorales', 'tb_logs'))\n",
    "\n",
    "log_dir = join(getcwd(), 'bach-chorales', 'tb_logs', time.strftime(\"run_%Y_%m_%d-%H_%M_%S\"))\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir = log_dir+'_loop_01', histogram_freq = 1)\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True, verbose=True)\n",
    "lr_plateau_callback = keras.callbacks.ReduceLROnPlateau(factor=.3, min_lr = lr_0/100, patience=9, verbose=1)\n",
    "\n",
    "# compile model, batch the datasets and train model\n",
    "\n",
    "# PROBABLY SHOULDN'T BATCH THE VALIDATION SET! longer sequences tend to provide better prediction...\n",
    "# also shouldn't randomly transpose validation set\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate = lr_0)\n",
    "model.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = 'accuracy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-anthony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 / current learning rate: 0.009999999776482582 / new learning rate: 0.009756097342909838\n",
      "Epoch 1/1000\n",
      " 1/54 [..............................] - ETA: 0s - loss: 4.0722 - accuracy: 0.0227WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      " 2/54 [>.............................] - ETA: 1:23 - loss: 4.0076 - accuracy: 0.0408WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 1.0459s vs `on_train_batch_end` time: 2.1561s). Check your callbacks.\n",
      "54/54 [==============================] - 70s 1s/step - loss: 3.1946 - accuracy: 0.1437 - val_loss: 2.6699 - val_accuracy: 0.2028\n",
      "epoch: 1 / current learning rate: 0.009756097570061684 / new learning rate: 0.009523809532679263\n",
      "Epoch 2/1000\n",
      "54/54 [==============================] - 70s 1s/step - loss: 2.1876 - accuracy: 0.3820 - val_loss: 1.4475 - val_accuracy: 0.5997\n",
      "epoch: 2 / current learning rate: 0.009523809887468815 / new learning rate: 0.009302325936597447\n",
      "Epoch 3/1000\n",
      "54/54 [==============================] - 70s 1s/step - loss: 1.5814 - accuracy: 0.5612 - val_loss: 1.1747 - val_accuracy: 0.6882\n",
      "epoch: 3 / current learning rate: 0.009302325546741486 / new learning rate: 0.009090909057042814\n",
      "Epoch 4/1000\n",
      "54/54 [==============================] - 72s 1s/step - loss: 1.3990 - accuracy: 0.6147 - val_loss: 1.0669 - val_accuracy: 0.7207\n",
      "epoch: 4 / current learning rate: 0.00909090880304575 / new learning rate: 0.008888888607422511\n",
      "Epoch 5/1000\n",
      "54/54 [==============================] - 71s 1s/step - loss: 1.2897 - accuracy: 0.6496 - val_loss: 0.9944 - val_accuracy: 0.7403\n",
      "epoch: 5 / current learning rate: 0.00888888817280531 / new learning rate: 0.0086956514733965\n",
      "Epoch 6/1000\n",
      "54/54 [==============================] - 71s 1s/step - loss: 1.2185 - accuracy: 0.6695 - val_loss: 0.9540 - val_accuracy: 0.7497\n",
      "epoch: 6 / current learning rate: 0.008695651777088642 / new learning rate: 0.008510637909491012\n",
      "Epoch 7/1000\n",
      "54/54 [==============================] - 73s 1s/step - loss: 1.1729 - accuracy: 0.6817 - val_loss: 0.9311 - val_accuracy: 0.7558\n",
      "epoch: 7 / current learning rate: 0.008510638028383255 / new learning rate: 0.008333333069458604\n",
      "Epoch 8/1000\n",
      "54/54 [==============================] - 72s 1s/step - loss: 1.1370 - accuracy: 0.6924 - val_loss: 0.9169 - val_accuracy: 0.7595\n",
      "epoch: 8 / current learning rate: 0.00833333283662796 / new learning rate: 0.00816326481955392\n",
      "Epoch 9/1000\n",
      "54/54 [==============================] - 72s 1s/step - loss: 1.1088 - accuracy: 0.6992 - val_loss: 0.8984 - val_accuracy: 0.7625\n",
      "epoch: 9 / current learning rate: 0.008163264952600002 / new learning rate: 0.007999999653548003\n",
      "Epoch 10/1000\n",
      "54/54 [==============================] - 73s 1s/step - loss: 1.0835 - accuracy: 0.7059 - val_loss: 0.8849 - val_accuracy: 0.7659\n",
      "epoch: 10 / current learning rate: 0.007999999448657036 / new learning rate: 0.007843136714369644\n",
      "Epoch 11/1000\n",
      "54/54 [==============================] - 72s 1s/step - loss: 1.0637 - accuracy: 0.7114 - val_loss: 0.8724 - val_accuracy: 0.7672\n",
      "epoch: 11 / current learning rate: 0.00784313678741455 / new learning rate: 0.007692307233810424\n",
      "Epoch 12/1000\n",
      "54/54 [==============================] - 72s 1s/step - loss: 1.0498 - accuracy: 0.7148 - val_loss: 0.8685 - val_accuracy: 0.7703\n",
      "epoch: 12 / current learning rate: 0.007692307233810425 / new learning rate: 0.007547169361474379\n",
      "Epoch 13/1000\n",
      "54/54 [==============================] - 72s 1s/step - loss: 1.0326 - accuracy: 0.7185 - val_loss: 0.8511 - val_accuracy: 0.7750\n",
      "epoch: 13 / current learning rate: 0.007547169458121061 / new learning rate: 0.0074074070607484475\n",
      "Epoch 14/1000\n",
      "54/54 [==============================] - 72s 1s/step - loss: 1.0230 - accuracy: 0.7217 - val_loss: 0.8477 - val_accuracy: 0.7743\n",
      "epoch: 14 / current learning rate: 0.007407407276332378 / new learning rate: 0.0072727271440354265\n",
      "Epoch 15/1000\n",
      "54/54 [==============================] - 71s 1s/step - loss: 1.0103 - accuracy: 0.7243 - val_loss: 0.8335 - val_accuracy: 0.7778\n",
      "epoch: 15 / current learning rate: 0.007272727321833372 / new learning rate: 0.007142857191086348\n",
      "Epoch 16/1000\n",
      "54/54 [==============================] - 70s 1s/step - loss: 0.9979 - accuracy: 0.7269 - val_loss: 0.8333 - val_accuracy: 0.7793\n",
      "epoch: 16 / current learning rate: 0.0071428571827709675 / new learning rate: 0.007017543898862704\n",
      "Epoch 17/1000\n",
      "54/54 [==============================] - 67s 1s/step - loss: 0.9903 - accuracy: 0.7285 - val_loss: 0.8236 - val_accuracy: 0.7819\n",
      "epoch: 17 / current learning rate: 0.007017544005066156 / new learning rate: 0.006896551867047775\n",
      "Epoch 18/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.9778 - accuracy: 0.7326 - val_loss: 0.8200 - val_accuracy: 0.7810\n",
      "epoch: 18 / current learning rate: 0.006896551698446274 / new learning rate: 0.006779660991692946\n",
      "Epoch 19/1000\n",
      "54/54 [==============================] - 67s 1s/step - loss: 0.9693 - accuracy: 0.7352 - val_loss: 0.8142 - val_accuracy: 0.7823\n",
      "epoch: 19 / current learning rate: 0.006779660936444998 / new learning rate: 0.006666666587504248\n",
      "Epoch 20/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.9684 - accuracy: 0.7346 - val_loss: 0.8105 - val_accuracy: 0.7829\n",
      "epoch: 20 / current learning rate: 0.006666666362434626 / new learning rate: 0.006557376749935698\n",
      "Epoch 21/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.9590 - accuracy: 0.7366 - val_loss: 0.8044 - val_accuracy: 0.7863\n",
      "epoch: 21 / current learning rate: 0.006557376589626074 / new learning rate: 0.006451612451083717\n",
      "Epoch 22/1000\n",
      "54/54 [==============================] - 67s 1s/step - loss: 0.9447 - accuracy: 0.7402 - val_loss: 0.7958 - val_accuracy: 0.7874\n",
      "epoch: 22 / current learning rate: 0.006451612338423729 / new learning rate: 0.006349205793369385\n",
      "Epoch 23/1000\n",
      "54/54 [==============================] - 67s 1s/step - loss: 0.9398 - accuracy: 0.7418 - val_loss: 0.7929 - val_accuracy: 0.7869\n",
      "epoch: 23 / current learning rate: 0.00634920597076416 / new learning rate: 0.006249999627470969\n",
      "Epoch 24/1000\n",
      "54/54 [==============================] - 72s 1s/step - loss: 0.9399 - accuracy: 0.7414 - val_loss: 0.7947 - val_accuracy: 0.7872\n",
      "epoch: 24 / current learning rate: 0.00624999962747097 / new learning rate: 0.00615384578704834\n",
      "Epoch 25/1000\n",
      "54/54 [==============================] - 73s 1s/step - loss: 0.9336 - accuracy: 0.7428 - val_loss: 0.7857 - val_accuracy: 0.7885\n",
      "epoch: 25 / current learning rate: 0.006153845693916082 / new learning rate: 0.006060605607644627\n",
      "Epoch 26/1000\n",
      "54/54 [==============================] - 67s 1s/step - loss: 0.9276 - accuracy: 0.7445 - val_loss: 0.7825 - val_accuracy: 0.7890\n",
      "epoch: 26 / current learning rate: 0.006060605403035879 / new learning rate: 0.005970148605975642\n",
      "Epoch 27/1000\n",
      "54/54 [==============================] - 67s 1s/step - loss: 0.9254 - accuracy: 0.7453 - val_loss: 0.7835 - val_accuracy: 0.7896\n",
      "epoch: 27 / current learning rate: 0.0059701488353312016 / new learning rate: 0.005882352528929272\n",
      "Epoch 28/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.9160 - accuracy: 0.7474 - val_loss: 0.7766 - val_accuracy: 0.7919\n",
      "epoch: 28 / current learning rate: 0.005882352590560913 / new learning rate: 0.005797101103741189\n",
      "Epoch 29/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.9139 - accuracy: 0.7467 - val_loss: 0.7737 - val_accuracy: 0.7927\n",
      "epoch: 29 / current learning rate: 0.005797101184725761 / new learning rate: 0.005714285453515393\n",
      "Epoch 30/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.9086 - accuracy: 0.7487 - val_loss: 0.7706 - val_accuracy: 0.7925\n",
      "epoch: 30 / current learning rate: 0.0057142856530845165 / new learning rate: 0.005633802756562199\n",
      "Epoch 31/1000\n",
      "54/54 [==============================] - 67s 1s/step - loss: 0.9068 - accuracy: 0.7484 - val_loss: 0.7719 - val_accuracy: 0.7935\n",
      "epoch: 31 / current learning rate: 0.005633802618831396 / new learning rate: 0.005555555360236515\n",
      "Epoch 32/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 66s 1s/step - loss: 0.9032 - accuracy: 0.7491 - val_loss: 0.7660 - val_accuracy: 0.7947\n",
      "epoch: 32 / current learning rate: 0.00555555522441864 / new learning rate: 0.005479451728193727\n",
      "Epoch 33/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8952 - accuracy: 0.7524 - val_loss: 0.7633 - val_accuracy: 0.7950\n",
      "epoch: 33 / current learning rate: 0.005479451734572649 / new learning rate: 0.005405405089510856\n",
      "Epoch 34/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8950 - accuracy: 0.7520 - val_loss: 0.7601 - val_accuracy: 0.7952\n",
      "epoch: 34 / current learning rate: 0.005405405070632696 / new learning rate: 0.0053333330030242605\n",
      "Epoch 35/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8915 - accuracy: 0.7529 - val_loss: 0.7590 - val_accuracy: 0.7962\n",
      "epoch: 35 / current learning rate: 0.005333332810550928 / new learning rate: 0.005263157378833153\n",
      "Epoch 36/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8898 - accuracy: 0.7538 - val_loss: 0.7562 - val_accuracy: 0.7967\n",
      "epoch: 36 / current learning rate: 0.0052631571888923645 / new learning rate: 0.005194804498127528\n",
      "Epoch 37/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8851 - accuracy: 0.7545 - val_loss: 0.7548 - val_accuracy: 0.7983\n",
      "epoch: 37 / current learning rate: 0.005194804631173611 / new learning rate: 0.005128204571799591\n",
      "Epoch 38/1000\n",
      "54/54 [==============================] - 67s 1s/step - loss: 0.8810 - accuracy: 0.7555 - val_loss: 0.7548 - val_accuracy: 0.7974\n",
      "epoch: 38 / current learning rate: 0.005128204356878996 / new learning rate: 0.0050632903776779955\n",
      "Epoch 39/1000\n",
      "54/54 [==============================] - 68s 1s/step - loss: 0.8808 - accuracy: 0.7555 - val_loss: 0.7511 - val_accuracy: 0.7980\n",
      "epoch: 39 / current learning rate: 0.005063290242105722 / new learning rate: 0.004999999114079401\n",
      "Epoch 40/1000\n",
      "54/54 [==============================] - 67s 1s/step - loss: 0.8774 - accuracy: 0.7557 - val_loss: 0.7504 - val_accuracy: 0.7981\n",
      "epoch: 40 / current learning rate: 0.004999998956918716 / new learning rate: 0.004938270574734535\n",
      "Epoch 41/1000\n",
      "54/54 [==============================] - 65s 1s/step - loss: 0.8725 - accuracy: 0.7577 - val_loss: 0.7454 - val_accuracy: 0.7997\n",
      "epoch: 41 / current learning rate: 0.004938270431011915 / new learning rate: 0.0048780476208776236\n",
      "Epoch 42/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8696 - accuracy: 0.7580 - val_loss: 0.7475 - val_accuracy: 0.7994\n",
      "epoch: 42 / current learning rate: 0.004878047853708267 / new learning rate: 0.0048192761928202155\n",
      "Epoch 43/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8668 - accuracy: 0.7583 - val_loss: 0.7457 - val_accuracy: 0.7991\n",
      "epoch: 43 / current learning rate: 0.00481927627697587 / new learning rate: 0.004761903940345205\n",
      "Epoch 44/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8671 - accuracy: 0.7582 - val_loss: 0.7420 - val_accuracy: 0.8002\n",
      "epoch: 44 / current learning rate: 0.004761904012411833 / new learning rate: 0.004705881612265811\n",
      "Epoch 45/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8634 - accuracy: 0.7594 - val_loss: 0.7405 - val_accuracy: 0.8003\n",
      "epoch: 45 / current learning rate: 0.004705881699919701 / new learning rate: 0.004651162145269472\n",
      "Epoch 46/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8615 - accuracy: 0.7604 - val_loss: 0.7408 - val_accuracy: 0.8009\n",
      "epoch: 46 / current learning rate: 0.0046511623077094555 / new learning rate: 0.004597700671988657\n",
      "Epoch 47/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8562 - accuracy: 0.7607 - val_loss: 0.7351 - val_accuracy: 0.8014\n",
      "epoch: 47 / current learning rate: 0.0045977006666362286 / new learning rate: 0.004545454068151725\n",
      "Epoch 48/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8539 - accuracy: 0.7614 - val_loss: 0.7369 - val_accuracy: 0.8024\n",
      "epoch: 48 / current learning rate: 0.0045454539358615875 / new learning rate: 0.0044943814197283116\n",
      "Epoch 49/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8535 - accuracy: 0.7621 - val_loss: 0.7334 - val_accuracy: 0.8026\n",
      "epoch: 49 / current learning rate: 0.004494381602853537 / new learning rate: 0.004444444029488497\n",
      "Epoch 50/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8523 - accuracy: 0.7627 - val_loss: 0.7352 - val_accuracy: 0.8016\n",
      "epoch: 50 / current learning rate: 0.004444444086402655 / new learning rate: 0.004395604041497131\n",
      "Epoch 51/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8491 - accuracy: 0.7636 - val_loss: 0.7326 - val_accuracy: 0.8029\n",
      "epoch: 51 / current learning rate: 0.004395604133605957 / new learning rate: 0.004347825827805893\n",
      "Epoch 52/1000\n",
      "54/54 [==============================] - 67s 1s/step - loss: 0.8488 - accuracy: 0.7623 - val_loss: 0.7284 - val_accuracy: 0.8037\n",
      "epoch: 52 / current learning rate: 0.004347825888544321 / new learning rate: 0.004301075072538468\n",
      "Epoch 53/1000\n",
      "54/54 [==============================] - 67s 1s/step - loss: 0.8459 - accuracy: 0.7629 - val_loss: 0.7263 - val_accuracy: 0.8044\n",
      "epoch: 53 / current learning rate: 0.004301074892282486 / new learning rate: 0.00425531877640714\n",
      "Epoch 54/1000\n",
      "54/54 [==============================] - 67s 1s/step - loss: 0.8434 - accuracy: 0.7637 - val_loss: 0.7218 - val_accuracy: 0.8060\n",
      "epoch: 54 / current learning rate: 0.00425531854853034 / new learning rate: 0.004210525721703705\n",
      "Epoch 55/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8393 - accuracy: 0.7648 - val_loss: 0.7103 - val_accuracy: 0.8076\n",
      "epoch: 55 / current learning rate: 0.004210525657981634 / new learning rate: 0.004166666015710993\n",
      "Epoch 56/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8359 - accuracy: 0.7653 - val_loss: 0.6954 - val_accuracy: 0.8103\n",
      "epoch: 56 / current learning rate: 0.004166665952652693 / new learning rate: 0.004123710633553181\n",
      "Epoch 57/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8282 - accuracy: 0.7671 - val_loss: 0.6900 - val_accuracy: 0.8100\n",
      "epoch: 57 / current learning rate: 0.004123710561543703 / new learning rate: 0.004081631882344276\n",
      "Epoch 58/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8255 - accuracy: 0.7673 - val_loss: 0.6863 - val_accuracy: 0.8115\n",
      "epoch: 58 / current learning rate: 0.004081632010638714 / new learning rate: 0.004040403404470646\n",
      "Epoch 59/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8231 - accuracy: 0.7684 - val_loss: 0.6853 - val_accuracy: 0.8120\n",
      "epoch: 59 / current learning rate: 0.004040403291583061 / new learning rate: 0.003999999258667231\n",
      "Epoch 60/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8207 - accuracy: 0.7670 - val_loss: 0.6874 - val_accuracy: 0.8107\n",
      "epoch: 60 / current learning rate: 0.003999999258667231 / new learning rate: 0.00396039530561112\n",
      "Epoch 61/1000\n",
      "54/54 [==============================] - 67s 1s/step - loss: 0.8138 - accuracy: 0.7705 - val_loss: 0.7579 - val_accuracy: 0.7966\n",
      "epoch: 61 / current learning rate: 0.003960395231842995 / new learning rate: 0.00392156782760924\n",
      "Epoch 62/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8439 - accuracy: 0.7612 - val_loss: 0.6886 - val_accuracy: 0.8109\n",
      "epoch: 62 / current learning rate: 0.003921567928045988 / new learning rate: 0.0038834944530164147\n",
      "Epoch 63/1000\n",
      "54/54 [==============================] - 68s 1s/step - loss: 0.8251 - accuracy: 0.7660 - val_loss: 0.6791 - val_accuracy: 0.8119\n",
      "epoch: 63 / current learning rate: 0.0038834945298731327 / new learning rate: 0.003846153236316661\n",
      "Epoch 64/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8131 - accuracy: 0.7684 - val_loss: 0.6804 - val_accuracy: 0.8122\n",
      "epoch: 64 / current learning rate: 0.003846153151243925 / new learning rate: 0.0038095231212320786\n",
      "Epoch 65/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8082 - accuracy: 0.7697 - val_loss: 0.6739 - val_accuracy: 0.8126\n",
      "epoch: 65 / current learning rate: 0.00380952307023108 / new learning rate: 0.0037735841733421076\n",
      "Epoch 66/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 68s 1s/step - loss: 0.7994 - accuracy: 0.7726 - val_loss: 0.6727 - val_accuracy: 0.8129\n",
      "epoch: 66 / current learning rate: 0.0037735842633992434 / new learning rate: 0.003738317120750652\n",
      "Epoch 67/1000\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.8008 - accuracy: 0.7726 - val_loss: 0.6668 - val_accuracy: 0.8141\n",
      "epoch: 67 / current learning rate: 0.003738317172974348 / new learning rate: 0.003703703125076437\n",
      "Epoch 68/1000\n",
      "54/54 [==============================] - 67s 1s/step - loss: 0.8067 - accuracy: 0.7697 - val_loss: 0.6778 - val_accuracy: 0.8121\n",
      "epoch: 68 / current learning rate: 0.003703703172504902 / new learning rate: 0.003669724244316784\n",
      "Epoch 69/1000\n",
      "54/54 [==============================] - 67s 1s/step - loss: 0.7977 - accuracy: 0.7719 - val_loss: 0.6757 - val_accuracy: 0.8124\n",
      "epoch: 69 / current learning rate: 0.0036697243340313435 / new learning rate: 0.0036363632037219678\n",
      "Epoch 70/1000\n",
      "54/54 [==============================] - 67s 1s/step - loss: 0.7932 - accuracy: 0.7732 - val_loss: 0.6683 - val_accuracy: 0.8141\n",
      "epoch: 70 / current learning rate: 0.0036363631952553988 / new learning rate: 0.0036036031664693144\n",
      "Epoch 71/1000\n",
      "54/54 [==============================] - 67s 1s/step - loss: 0.7892 - accuracy: 0.7742 - val_loss: 0.6603 - val_accuracy: 0.8146\n",
      "epoch: 71 / current learning rate: 0.0036036032252013683 / new learning rate: 0.0035714281964049272\n",
      "Epoch 72/1000\n",
      "54/54 [==============================] - 67s 1s/step - loss: 0.7866 - accuracy: 0.7743 - val_loss: 0.6625 - val_accuracy: 0.8150\n",
      "epoch: 72 / current learning rate: 0.0035714281257241964 / new learning rate: 0.0035398225670894685\n",
      "Epoch 73/1000\n",
      "54/54 [==============================] - 67s 1s/step - loss: 0.7813 - accuracy: 0.7761 - val_loss: 0.6599 - val_accuracy: 0.8153\n",
      "epoch: 73 / current learning rate: 0.003539822530001402 / new learning rate: 0.0035087714551768285\n",
      "Epoch 74/1000\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.7870 - accuracy: 0.7745 - val_loss: 0.6605 - val_accuracy: 0.8158\n",
      "epoch: 74 / current learning rate: 0.003508771536871791 / new learning rate: 0.003478260480029427\n",
      "Epoch 75/1000\n",
      "54/54 [==============================] - 73s 1s/step - loss: 0.7858 - accuracy: 0.7744 - val_loss: 0.6589 - val_accuracy: 0.8150\n",
      "epoch: 75 / current learning rate: 0.003478260478004813 / new learning rate: 0.0034482754738840824\n",
      "Epoch 76/1000\n",
      "54/54 [==============================] - 75s 1s/step - loss: 0.7769 - accuracy: 0.7760 - val_loss: 0.6555 - val_accuracy: 0.8162\n",
      "epoch: 76 / current learning rate: 0.0034482753835618496 / new learning rate: 0.0034188029443861074\n",
      "Epoch 77/1000\n",
      "54/54 [==============================] - 73s 1s/step - loss: 0.7687 - accuracy: 0.7780 - val_loss: 0.6575 - val_accuracy: 0.8153\n",
      "epoch: 77 / current learning rate: 0.003418802982196212 / new learning rate: 0.00338983007556743\n",
      "Epoch 78/1000\n",
      "54/54 [==============================] - 73s 1s/step - loss: 0.7689 - accuracy: 0.7777 - val_loss: 0.6496 - val_accuracy: 0.8183\n",
      "epoch: 78 / current learning rate: 0.0033898300025612116 / new learning rate: 0.0033613440361531344\n",
      "Epoch 79/1000\n",
      "54/54 [==============================] - 74s 1s/step - loss: 0.7713 - accuracy: 0.7781 - val_loss: 0.6493 - val_accuracy: 0.8170\n",
      "epoch: 79 / current learning rate: 0.0033613441046327353 / new learning rate: 0.0033333329037607963\n",
      "Epoch 80/1000\n",
      "54/54 [==============================] - 72s 1s/step - loss: 0.7626 - accuracy: 0.7793 - val_loss: 0.6610 - val_accuracy: 0.8148\n",
      "epoch: 80 / current learning rate: 0.003333332948386669 / new learning rate: 0.0033057847422016556\n",
      "Epoch 81/1000\n",
      "54/54 [==============================] - 72s 1s/step - loss: 0.7664 - accuracy: 0.7785 - val_loss: 0.6507 - val_accuracy: 0.8175\n",
      "epoch: 81 / current learning rate: 0.003305784659460187 / new learning rate: 0.003278688063890841\n",
      "Epoch 82/1000\n",
      "54/54 [==============================] - 72s 1s/step - loss: 0.7659 - accuracy: 0.7796 - val_loss: 0.6454 - val_accuracy: 0.8176\n",
      "epoch: 82 / current learning rate: 0.0032786880619823933 / new learning rate: 0.003252032061478471\n",
      "Epoch 83/1000\n",
      "54/54 [==============================] - 72s 1s/step - loss: 0.7633 - accuracy: 0.7791 - val_loss: 0.6553 - val_accuracy: 0.8167\n",
      "epoch: 83 / current learning rate: 0.0032520319800823927 / new learning rate: 0.0032258059157268897\n",
      "Epoch 84/1000\n",
      "54/54 [==============================] - 71s 1s/step - loss: 0.8297 - accuracy: 0.7646 - val_loss: 0.6742 - val_accuracy: 0.8135\n",
      "epoch: 84 / current learning rate: 0.003225805936381221 / new learning rate: 0.003199999488890171\n",
      "Epoch 85/1000\n",
      "54/54 [==============================] - 69s 1s/step - loss: 0.7911 - accuracy: 0.7730 - val_loss: 0.6744 - val_accuracy: 0.8124\n",
      "epoch: 85 / current learning rate: 0.003199999453499913 / new learning rate: 0.0031746026324403903\n",
      "Epoch 86/1000\n",
      "54/54 [==============================] - 69s 1s/step - loss: 0.7986 - accuracy: 0.7709 - val_loss: 0.6592 - val_accuracy: 0.8157\n",
      "epoch: 86 / current learning rate: 0.0031746025197207928 / new learning rate: 0.003149605649486771\n",
      "Epoch 87/1000\n",
      "54/54 [==============================] - 70s 1s/step - loss: 0.7779 - accuracy: 0.7762 - val_loss: 0.6547 - val_accuracy: 0.8168\n",
      "epoch: 87 / current learning rate: 0.0031496055889874697 / new learning rate: 0.0031249992953235046\n",
      "Epoch 88/1000\n",
      "54/54 [==============================] - 70s 1s/step - loss: 0.7820 - accuracy: 0.7754 - val_loss: 0.6520 - val_accuracy: 0.8173\n",
      "epoch: 88 / current learning rate: 0.0031249993480741978 / new learning rate: 0.0031007745469263355\n",
      "Epoch 89/1000\n",
      "54/54 [==============================] - 82s 2s/step - loss: 0.7773 - accuracy: 0.7755 - val_loss: 0.6488 - val_accuracy: 0.8173\n",
      "epoch: 89 / current learning rate: 0.003100774483755231 / new learning rate: 0.0030769223723417292\n",
      "Epoch 90/1000\n",
      "54/54 [==============================] - 75s 1s/step - loss: 0.7674 - accuracy: 0.7796 - val_loss: 0.6456 - val_accuracy: 0.8183\n",
      "epoch: 90 / current learning rate: 0.003076922381296754 / new learning rate: 0.0030534344241876185\n",
      "Epoch 91/1000\n",
      "54/54 [==============================] - 76s 1s/step - loss: 0.7646 - accuracy: 0.7802 - val_loss: 0.6441 - val_accuracy: 0.8189\n",
      "epoch: 91 / current learning rate: 0.0030534344259649515 / new learning rate: 0.0030303023469803684\n",
      "Epoch 92/1000\n",
      "54/54 [==============================] - 72s 1s/step - loss: 0.7587 - accuracy: 0.7803 - val_loss: 0.6454 - val_accuracy: 0.8175\n",
      "epoch: 92 / current learning rate: 0.0030303022358566523 / new learning rate: 0.003007518008519384\n",
      "Epoch 93/1000\n",
      "54/54 [==============================] - 72s 1s/step - loss: 0.7588 - accuracy: 0.7805 - val_loss: 0.6429 - val_accuracy: 0.8186\n",
      "epoch: 93 / current learning rate: 0.003007517894729972 / new learning rate: 0.002985073731336465\n",
      "Epoch 94/1000\n",
      "54/54 [==============================] - 69s 1s/step - loss: 0.7527 - accuracy: 0.7818 - val_loss: 0.6441 - val_accuracy: 0.8190\n",
      "epoch: 94 / current learning rate: 0.00298507371917367 / new learning rate: 0.0029629620619946056\n",
      "Epoch 95/1000\n",
      "54/54 [==============================] - 70s 1s/step - loss: 0.7496 - accuracy: 0.7829 - val_loss: 0.6477 - val_accuracy: 0.8183\n",
      "epoch: 95 / current learning rate: 0.0029629620257765055 / new learning rate: 0.0029411755402928546\n",
      "Epoch 96/1000\n",
      "54/54 [==============================] - 72s 1s/step - loss: 0.7496 - accuracy: 0.7821 - val_loss: 0.6432 - val_accuracy: 0.8178\n",
      "epoch: 96 / current learning rate: 0.0029411755967885256 / new learning rate: 0.0029197071617754703\n",
      "Epoch 97/1000\n",
      "54/54 [==============================] - 72s 1s/step - loss: 0.7457 - accuracy: 0.7835 - val_loss: 0.6419 - val_accuracy: 0.8192\n",
      "epoch: 97 / current learning rate: 0.002919707214459777 / new learning rate: 0.0028985499158042712\n",
      "Epoch 98/1000\n",
      "54/54 [==============================] - 73s 1s/step - loss: 0.7453 - accuracy: 0.7840 - val_loss: 0.6415 - val_accuracy: 0.8195\n",
      "epoch: 98 / current learning rate: 0.0028985498938709497 / new learning rate: 0.0028776970169366264\n",
      "Epoch 99/1000\n",
      "54/54 [==============================] - 73s 1s/step - loss: 0.7433 - accuracy: 0.7836 - val_loss: 0.6366 - val_accuracy: 0.8191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 99 / current learning rate: 0.002877697115764022 / new learning rate: 0.0028571421363657074\n",
      "Epoch 100/1000\n",
      "54/54 [==============================] - 71s 1s/step - loss: 0.7418 - accuracy: 0.7845 - val_loss: 0.6356 - val_accuracy: 0.8200\n",
      "epoch: 100 / current learning rate: 0.0028571421280503273 / new learning rate: 0.0028368787087024528\n",
      "Epoch 101/1000\n",
      "54/54 [==============================] - 73s 1s/step - loss: 0.7388 - accuracy: 0.7848 - val_loss: 0.6343 - val_accuracy: 0.8199\n",
      "epoch: 101 / current learning rate: 0.0028368786443024874 / new learning rate: 0.002816900625680639\n",
      "Epoch 102/1000\n",
      "54/54 [==============================] - 70s 1s/step - loss: 0.7377 - accuracy: 0.7853 - val_loss: 0.6367 - val_accuracy: 0.8197\n",
      "epoch: 102 / current learning rate: 0.002816900610923767 / new learning rate: 0.002797202005252971\n",
      "Epoch 103/1000\n",
      "54/54 [==============================] - 72s 1s/step - loss: 0.7361 - accuracy: 0.7856 - val_loss: 0.6336 - val_accuracy: 0.8200\n",
      "epoch: 103 / current learning rate: 0.0027972019743174314 / new learning rate: 0.002777776960606894\n",
      "Epoch 104/1000\n",
      "54/54 [==============================] - 76s 1s/step - loss: 0.7351 - accuracy: 0.7849 - val_loss: 0.6313 - val_accuracy: 0.8208\n",
      "epoch: 104 / current learning rate: 0.002777776913717389 / new learning rate: 0.002758619831553821\n",
      "Epoch 105/1000\n",
      "54/54 [==============================] - 85s 2s/step - loss: 0.7365 - accuracy: 0.7855 - val_loss: 0.6331 - val_accuracy: 0.8205\n",
      "epoch: 105 / current learning rate: 0.0027586198411881924 / new learning rate: 0.0027397251847416978\n",
      "Epoch 106/1000\n",
      "54/54 [==============================] - 82s 2s/step - loss: 0.7326 - accuracy: 0.7865 - val_loss: 0.6361 - val_accuracy: 0.8198\n",
      "epoch: 106 / current learning rate: 0.0027397251687943935 / new learning rate: 0.0027210875826121188\n",
      "Epoch 107/1000\n",
      "54/54 [==============================] - 73s 1s/step - loss: 0.7340 - accuracy: 0.7854 - val_loss: 0.6326 - val_accuracy: 0.8205\n",
      "epoch: 107 / current learning rate: 0.0027210875414311886 / new learning rate: 0.002702701814799897\n",
      "Epoch 108/1000\n",
      "54/54 [==============================] - 71s 1s/step - loss: 0.7291 - accuracy: 0.7870 - val_loss: 0.6384 - val_accuracy: 0.8197\n",
      "epoch: 108 / current learning rate: 0.002702701836824417 / new learning rate: 0.00268456289832224\n",
      "Epoch 109/1000\n",
      "54/54 [==============================] - 76s 1s/step - loss: 0.7329 - accuracy: 0.7859 - val_loss: 0.6364 - val_accuracy: 0.8200\n",
      "epoch: 109 / current learning rate: 0.0026845629326999187 / new learning rate: 0.002666665846481919\n",
      "Epoch 110/1000\n",
      "23/54 [===========>..................] - ETA: 41s - loss: 0.7190 - accuracy: 0.7911"
     ]
    }
   ],
   "source": [
    "history = model.fit(train.batch(sequence_length).batch(batch_size, drop_remainder=True).cache().prefetch(16),\n",
    "                    epochs = 1000,\n",
    "                    verbose = 1, # 1 : progress bar / 2 : one entry per epoch\n",
    "                    validation_data = valid.batch(sequence_length).batch(batch_size, drop_remainder=True).cache().prefetch(1),\n",
    "                    callbacks = [lr_scheduler_callback, lr_plateau_callback, tensorboard_callback, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(train.batch(sequence_length).batch(batch_size, drop_remainder=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-foster",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(valid.batch(sequence_length).batch(batch_size, drop_remainder=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test.batch(sequence_length).batch(batch_size, drop_remainder=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-sculpture",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-sheep",
   "metadata": {},
   "source": [
    "### Generating new music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# music generation\n",
    "\n",
    "Length = 800 # number of new notes on all voices, actual piece length is Length/4\n",
    "Temperature = 1.0 # not really temperature (which should be applied before softmax), but works similarly (smaller values are more adventurous, larger values are more conservative)\n",
    "\n",
    "song = []\n",
    "song_encoded = one_hot_encoding(-1).reshape((1,1,input_dim)).repeat(16, axis=1)\n",
    "song_encoded.shape\n",
    "\n",
    "new_note = -1\n",
    "new_note_encoding = one_hot_encoding(new_note).reshape((1,1,input_dim))\n",
    "\n",
    "# music generation loop\n",
    "\n",
    "for _ in range(Length):\n",
    "    p = model.predict(song_encoded)[:,-1,:].flatten() # the model is reading the entire chorale up to this point, which makes it very slow for long sequences. This should be improved, providing a limited number os samples for each new note or making the network remember the last state.\n",
    "    p = p**Temperature / (p**Temperature).sum()\n",
    "    \n",
    "    out = np.random.choice(input_dim, p=p)\n",
    "    new_note_encoding = np.zeros((1,1,input_dim))\n",
    "    new_note_encoding[0,0,out] = 1\n",
    "    new_note = one_hot_decoding(new_note_encoding)\n",
    "\n",
    "    song.append(new_note)\n",
    "    extended_song = np.zeros((song_encoded.shape[0], song_encoded.shape[1]+1, song_encoded.shape[2]))\n",
    "    extended_song[:,:-1,:] = song_encoded\n",
    "    extended_song[:,-1,:] = new_note_encoding\n",
    "    song_encoded = extended_song\n",
    "\n",
    "# print chorale as np array    \n",
    "np.array(song).reshape((-1,4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip entries before beginning\n",
    "\n",
    "first_note_position = 0\n",
    "for position, note in enumerate(song):\n",
    "    if note != -1:\n",
    "        first_note_position = position\n",
    "        break\n",
    "\n",
    "new_chorale = np.array(song[first_note_position : -4 + (first_note_position % 4)]).reshape(-1,4)\n",
    "        \n",
    "# clip entries after eof\n",
    "\n",
    "first_eof = (new_chorale.min(axis = 1) == -1).astype('int').argmax()\n",
    "if first_eof > 0:\n",
    "    new_chorale = new_chorale[:first_eof, :]\n",
    "        \n",
    "# convert finished product to data frame\n",
    "\n",
    "new_chorale = pd.DataFrame(new_chorale, columns=['note1','note2','note3','note4'])\n",
    "new_chorale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# midi encoding with legato (attack only when new note is different from current one)\n",
    "\n",
    "new_chorale_midi = mido.MidiFile(type=1)\n",
    "\n",
    "time_unit = 360\n",
    "instrument = 52 # choir aahs\n",
    "volume = [50, 50, 60, 75] # volume for each channel\n",
    "\n",
    "for channel in range(4):\n",
    "    track = mido.MidiTrack()\n",
    "    track.append(mido.Message('program_change', channel = channel, program = instrument, time = 0))\n",
    "    previous_note = 0\n",
    "    steps = 1\n",
    "    \n",
    "    for pos, note in enumerate(new_chorale.iloc[:, channel]):\n",
    "        if note == previous_note:\n",
    "            steps += 1\n",
    "        if note != previous_note:\n",
    "            if pos != 0:\n",
    "                track.append(mido.Message('note_off', channel=channel, note=previous_note, time = steps * time_unit))\n",
    "            track.append(mido.Message('note_on', channel=channel, note=note, velocity=volume[channel], time=0))\n",
    "            previous_note = note\n",
    "            steps = 1\n",
    "            \n",
    "    new_chorale_midi.tracks.append(track)\n",
    "    \n",
    "filename = 'new_chorale_' + time.strftime(\"_%Y_%m_%d-%H_%M_%S\") + '.mid'\n",
    "filepath = join(getcwd(), 'bach-chorales', 'new')\n",
    "\n",
    "new_chorale_midi.save(join(filepath, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-toner",
   "metadata": {},
   "source": [
    "### Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what are the lowest and hightest notes in the corpus\n",
    "\n",
    "#files = []\n",
    "#partition = ['train', 'valid', 'test']\n",
    "\n",
    "#for p in partition:\n",
    "#    filename = listdir(join(getcwd(), 'bach-chorales', p))\n",
    "#    for file in filename:\n",
    "#        files.append(join(getcwd(), 'bach-chorales', p, file))\n",
    "\n",
    "#pitch_range = []        \n",
    "\n",
    "#for file in files:\n",
    "#    df = pd.read_csv(file)\n",
    "#    df = df.where(df != 0, other = None)\n",
    "#    pitch_range.append((df.min().min(), df.max().max()))\n",
    "\n",
    "#np.array(pitch_range).min(), np.array(pitch_range).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1 - .46->.34 @train / .66->.60 @valid / .67->.62 @test\n",
    "#model = keras.models.Sequential()\n",
    "#model.add(keras.layers.GRU(input_dim*2, dropout = .1, recurrent_dropout = .1, input_shape = [None, input_dim], return_sequences = True))\n",
    "#model.add(keras.layers.GRU(input_dim*2, dropout = .1, recurrent_dropout = .1, return_sequences = True))\n",
    "#model.add(keras.layers.GRU(input_dim*2, dropout = .1, recurrent_dropout = .1, return_sequences = True))\n",
    "#model.add(keras.layers.Dense(input_dim, kernel_regularizer = keras.regularizers.l2(l2=0)))\n",
    "#model.add(keras.layers.Dropout(.1))\n",
    "#model.add(keras.layers.Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-consistency",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fossil-moses",
   "metadata": {},
   "source": [
    "##### Second learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second learning loop\n",
    "\n",
    "#learning_rate = 0.001 # 0.01 -> 0.001 -> 0.0001\n",
    "#sequence_length = 256 # 256\n",
    "#batch_size = 16 # 16 -> 16 -> 64\n",
    "\n",
    "#optimizer = keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "\n",
    "#tensorboard_callback = keras.callbacks.TensorBoard(log_dir = log_dir+'_loop_02', histogram_freq = 1)\n",
    "\n",
    "#model.compile(optimizer=optimizer, loss = 'categorical_crossentropy')\n",
    "\n",
    "# PROBABLY SHOULDN'T BATCH THE VALIDATION SET! longer sequences tend to provide better prediction...\n",
    "\n",
    "#history = model.fit(train.batch(sequence_length).batch(batch_size, drop_remainder=True).cache().prefetch(16),\n",
    "#                    epochs = 1000,\n",
    "#                    validation_data = valid.batch(sequence_length).batch(batch_size, drop_remainder=True).cache().prefetch(1),\n",
    "#                    callbacks = [early_stopping, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.evaluate(train.batch(sequence_length).batch(batch_size, drop_remainder=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-vertical",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.evaluate(valid.batch(sequence_length).batch(batch_size, drop_remainder=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.evaluate(test.batch(sequence_length).batch(batch_size, drop_remainder=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-burke",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-fabric",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-repository",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
